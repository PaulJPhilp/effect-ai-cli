---
provider: anthropic
model: claude-3-haiku
parameters:
  temperature: 0.8
  max_tokens: 200
---

# Streaming Test Prompt

This prompt is designed to test streaming capabilities of the LLM service.

## Instructions

Please provide a response that demonstrates streaming behavior by including:

1. **Progressive Output**: The response should naturally flow in chunks
2. **Real-time Feel**: Each section should feel like it's being generated in real-time
3. **Coherent Structure**: Despite streaming, maintain logical coherence

## Response Structure

Please generate a short story about a developer discovering Effect patterns:

- **Beginning**: Setup the scenario
- **Middle**: Describe the discovery process
- **End**: Show the resolution

## Example Story Flow

"As the developer sat at their desk, they noticed something interesting..."

[Pause for effect]

"The code seemed to have a pattern they hadn't seen before..."

[Build suspense]

"Then they realized - this was the Effect pattern they'd been looking for!"

## Technical Details

Please ensure the response:
- Uses TypeScript examples where appropriate
- References async/await concepts
- Mentions functional programming benefits
- Includes practical code examples
